{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# I. Fitting Data (Python)\nEste notebook resuelve la Parte I del trabajo, simulando los datos y entrenando distintas redes neuronales con `scikit-learn`.\nLas imágenes se guardan con sufijo `_python` para diferenciarlas de las versiones en R y Julia."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de semilla para reproducibilidad\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de datos\n",
    "n = 300\n",
    "x = np.linspace(0, 2 * np.pi, n)\n",
    "epsilon = np.random.normal(0, 0.1, n)\n",
    "y = np.sin(x) + epsilon\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.scatter(x, y, s=15, color='blue', label='Datos simulados')\n",
    "plt.title('Simulación de datos: y = sin(x) + ε')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('simulacion_python.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de redes neuronales con distintas funciones de activación\n",
    "X = x.reshape(-1, 1)\n",
    "y_true = y\n",
    "activations = ['logistic', 'tanh', 'relu']\n",
    "models = {}\n",
    "errors = {}\n",
    "\n",
    "for act in activations:\n",
    "    print(f'Entrenando red con activación: {act}')\n",
    "    model = MLPRegressor(hidden_layer_sizes=(50,50,50), activation=act, solver='adam', max_iter=3000, random_state=42)\n",
    "    model.fit(X, y_true)\n",
    "    models[act] = model\n",
    "    y_pred = model.predict(X)\n",
    "    errors[act] = mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico comparativo de activaciones\n",
    "x_grid = np.linspace(0, 2*np.pi, 500).reshape(-1,1)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, y, s=15, color='gray', label='Datos reales')\n",
    "for act in activations:\n",
    "    y_pred = models[act].predict(x_grid)\n",
    "    plt.plot(x_grid, y_pred, label=f'NN ({act})', linewidth=2)\n",
    "plt.title('Ajuste de NNs con diferentes funciones de activación')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparacion_activaciones_python.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal mixta (relu -> tanh -> logistic)\n",
    "m1 = MLPRegressor(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=3000, random_state=42)\n",
    "m1.fit(X, y_true)\n",
    "y_stage1 = m1.predict(X).reshape(-1,1)\n",
    "\n",
    "m2 = MLPRegressor(hidden_layer_sizes=(50,), activation='tanh', solver='adam', max_iter=3000, random_state=42)\n",
    "m2.fit(y_stage1, y_true)\n",
    "y_stage2 = m2.predict(y_stage1).reshape(-1,1)\n",
    "\n",
    "m3 = MLPRegressor(hidden_layer_sizes=(50,), activation='logistic', solver='adam', max_iter=3000, random_state=42)\n",
    "m3.fit(y_stage2, y_true)\n",
    "y_pred_mix = m3.predict(y_stage2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de comparación incluyendo la red mixta\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x, y, s=15, color='gray', label='Datos reales')\n",
    "for act in activations:\n",
    "    plt.plot(x_grid, models[act].predict(x_grid), label=f'NN ({act})')\n",
    "plt.plot(x, y_pred_mix, '--', color='black', linewidth=2, label='NN (relu-tanh-logistic)')\n",
    "plt.title('Comparación: NNs con distintas funciones de activación')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('red_mixta_python.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# II. Learning-rate\nEl learning rate (tasa de aprendizaje) controla qué tan grandes son los pasos que da el modelo al ajustar los pesos. Un valor demasiado bajo hace el entrenamiento muy lento; uno muy alto puede impedir la convergencia y causar oscilaciones."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "y_true = y\n",
    "activation_best = 'tanh'\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "layer_configs = [(50,), (50,50), (50,50,50)]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in layer_configs:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(x, y, s=15, color='gray', label='Datos reales')\n",
    "\n",
    "    print(f'\\nEvaluando red con {len(layers)} capa(s) oculta(s)')\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        model = MLPRegressor(hidden_layer_sizes=layers,\n",
    "                             activation=activation_best,\n",
    "                             solver='adam',\n",
    "                             learning_rate_init=lr,\n",
    "                             max_iter=3000,\n",
    "                             random_state=42)\n",
    "        model.fit(X, y_true)\n",
    "        y_pred = model.predict(X)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        results.append({'Capas': len(layers), 'LearningRate': lr, 'MSE': mse})\n",
    "\n",
    "        y_pred_plot = model.predict(np.linspace(0, 2*np.pi, 500).reshape(-1,1))\n",
    "        plt.plot(np.linspace(0, 2*np.pi, 500), y_pred_plot, label=f'lr={lr}', linewidth=2)\n",
    "\n",
    "    plt.title(f'Ajuste NN con {len(layers)} capa(s) oculta(s) (activación={activation_best})')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'learningrate_{len(layers)}capas_python.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame(results)\n",
    "df_lr.to_csv('learningrate_resultados_python.csv', index=False)\n",
    "print('Resumen de errores (MSE):')\n",
    "print(df_lr.pivot_table(values='MSE', index='LearningRate', columns='Capas'))\n",
    "\n",
    "print('Conclusión: A tasas de aprendizaje muy bajas (0.0001), el modelo aprende lentamente y puede subajustar. A tasas intermedias (0.001 o 0.01), el ajuste mejora notablemente. A tasas altas (0.1), el entrenamiento se vuelve inestable. Generalmente, mientras más capas tenga la red, más sensible se vuelve al learning rate alto.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
