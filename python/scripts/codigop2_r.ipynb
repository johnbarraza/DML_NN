{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aff8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 0. Instalación y Carga de Paquetes\n",
    "# --------------------------------------------------\n",
    "\n",
    "# install.packages(c(\"readr\", \"dplyr\", \"hdm\", \"glmnet\", \"randomForest\", \"sandwich\", \"nnet\"))\n",
    "\n",
    "library(readr)       # Para leer el archivo .dat\n",
    "library(dplyr)       # Para manipulación de datos (filter, mutate)\n",
    "library(hdm)         # Para rlasso (como en tu lab)\n",
    "library(glmnet)      # Para OLS, Logit y Lasso Logit\n",
    "library(randomForest)# Para Random Forest\n",
    "library(sandwich)    # Para SE robustos (vcovHC)\n",
    "library(nnet)        # Para Redes Neuronales\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Carga y Limpieza de Datos (Parte I)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Nombres de las columnas del archivo .dat\n",
    "nombres <- c(\n",
    "    \"abdt\", \"tg\", \"inuidur1\", \"inuidur2\", \"female\", \"black\", \"hispanic\", \n",
    "    \"othrace\", \"dep\", \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"recall\", \n",
    "    \"agelt35\", \"agegt54\", \"durable\", \"nondurable\", \"lusd\", \"husd\", \"muld\"\n",
    ")\n",
    "\n",
    "# Descargar y leer el archivo de datos\n",
    "url <- \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/penn_jae.dat\"\n",
    "df <- read_delim(url, delim = \" \", col_names = nombres, skip = 1, trim_ws = TRUE)\n",
    "\n",
    "# Aplicar la limpieza y creación de variables de tu notebook de Python\n",
    "df_cleaned <- df %>%\n",
    "  filter(tg == 0 | tg == 4) %>%\n",
    "  mutate(\n",
    "    # II. Definir 'T4' (d, tratamiento)\n",
    "    T4 = as.integer(tg == 4),\n",
    "    \n",
    "    # III. Definir 'y' (resultado)\n",
    "    y = log(inuidur1),\n",
    "    \n",
    "    # IV. Crear dummies para 'dep'\n",
    "    dep_1 = as.integer(dep == 1),\n",
    "    dep_2 = as.integer(dep == 2)\n",
    "  ) %>%\n",
    "  # Manejar log(0) = -Inf, similar a como lo hiciste en Python\n",
    "  filter(is.finite(y))\n",
    "\n",
    "print(paste(\"Datos después de limpiar:\", nrow(df_cleaned)))\n",
    "\n",
    "# V. Definir x (controles), y (resultado), d (tratamiento)\n",
    "feature_list <- c(\n",
    "    \"female\", \"black\", \"othrace\",\n",
    "    \"dep_1\", \"dep_2\",\n",
    "    \"q2\", \"q3\", \"q4\", \"q5\", \"q6\",\n",
    "    \"recall\", \"agelt35\", \"agegt54\",\n",
    "    \"durable\", \"nondurable\", \"lusd\", \"husd\"\n",
    ")\n",
    "\n",
    "x <- as.matrix(df_cleaned[, feature_list])\n",
    "y <- as.matrix(df_cleaned$y)\n",
    "d <- as.matrix(df_cleaned$T4)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Funciones DML (Parte II y III)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# ---\n",
    "# Función DML con Cross-Fitting (Parte II)\n",
    "# Basada en tu lab, pero modificada para manejar\n",
    "# los diferentes 'tipos' de predicción (ej. 'prob' para RF, 'response' para glmnet)\n",
    "# ---\n",
    "DML.for.PLM <- function(x, d, y, dreg, yreg, nfold=10, d_pred_type=\"response\", y_pred_type=\"response\") {\n",
    "  nobs <- nrow(x)\n",
    "  # Crear pliegues (folds)\n",
    "  foldid <- rep.int(1:nfold, times = ceiling(nobs/nfold))[sample.int(nobs)]\n",
    "  I <- split(1:nobs, foldid)\n",
    "  ytil <- dtil <- rep(NA, nobs)\n",
    "  \n",
    "  cat(\"Fold: \")\n",
    "  for(b in 1:length(I)){\n",
    "    # Ajustar modelos en los pliegues de entrenamiento (todos menos 'b')\n",
    "    dfit <- dreg(x[-I[[b]],], d[-I[[b]]])\n",
    "    yfit <- yreg(x[-I[[b]],], y[-I[[b]]])\n",
    "    \n",
    "    # Predecir en el pliegue de prueba ('b')\n",
    "    \n",
    "    # Manejar la predicción de D (tratamiento)\n",
    "    if (d_pred_type == \"prob\") {\n",
    "        # Para randomForest, 'prob' da 2 columnas (prob 0, prob 1). Queremos la segunda.\n",
    "        dhat <- predict(dfit, x[I[[b]],], type=\"prob\")[, 2]\n",
    "    } else if (d_pred_type == \"raw\" && inherits(dfit, \"nnet\")) {\n",
    "        # Para nnet, 'raw' da probabilidades. Si es binario, puede dar 1 o 2 columnas.\n",
    "        pred_raw <- predict(dfit, x[I[[b]],], type=\"raw\")\n",
    "        dhat <- if (ncol(pred_raw) == 2) pred_raw[, 2] else pred_raw[, 1]\n",
    "    } else {\n",
    "        # Para glmnet (logit y lasso), 'response' da la probabilidad P(D=1)\n",
    "        dhat <- predict(dfit, x[I[[b]],], type=d_pred_type)\n",
    "    }\n",
    "    \n",
    "    # Manejar la predicción de Y (resultado)\n",
    "    yhat <- predict(yfit, x[I[[b]],], type=y_pred_type)\n",
    "    \n",
    "    # Calcular residuales\n",
    "    dtil[I[[b]]] <- (d[I[[b]]] - dhat)\n",
    "    ytil[I[[b]]] <- (y[I[[b]]] - yhat)\n",
    "    cat(b, \" \")\n",
    "  }\n",
    "  \n",
    "  # Regresión final de residuales\n",
    "  rfit <- lm(ytil ~ dtil)\n",
    "  coef.est <- coef(rfit)[2]\n",
    "  se <- sqrt(vcovHC(rfit)[2,2]) # Error estándar robusto\n",
    "  \n",
    "  cat(sprintf(\"\\nCoef (SE) = %g (%g)\\n\", coef.est, se))\n",
    "  return( list(coef.est=coef.est , se=se, dtil=dtil, ytil=ytil) )\n",
    "}\n",
    "\n",
    "# ---\n",
    "# Función DML Naive (Sin Cross-Fitting) (Parte III)\n",
    "# ---\n",
    "DML.naive <- function(x, d, y, dreg, yreg, d_pred_type=\"response\", y_pred_type=\"response\") {\n",
    "  \n",
    "  # 1. Entrenar y predecir Y (en muestra)\n",
    "  yfit <- yreg(x, y)\n",
    "  yhat <- predict(yfit, x, type=y_pred_type)\n",
    "  \n",
    "  # 2. Entrenar y predecir D (en muestra)\n",
    "  dfit <- dreg(x, d)\n",
    "  \n",
    "  # Manejar la predicción de D (tratamiento)\n",
    "  if (d_pred_type == \"prob\") {\n",
    "      dhat <- predict(dfit, x, type=\"prob\")[, 2]\n",
    "  } else if (d_pred_type == \"raw\" && inherits(dfit, \"nnet\")) {\n",
    "      pred_raw <- predict(dfit, x, type=\"raw\")\n",
    "      dhat <- if (ncol(pred_raw) == 2) pred_raw[, 2] else pred_raw[, 1]\n",
    "  } else {\n",
    "      dhat <- predict(dfit, x, type=d_pred_type)\n",
    "  }\n",
    "\n",
    "  # 3. Calcular residuales\n",
    "  dtil <- (d - dhat)\n",
    "  ytil <- (y - yhat)\n",
    "  \n",
    "  # 4. Regresión final\n",
    "  rfit <- lm(ytil ~ dtil)\n",
    "  coef.est <- coef(rfit)[2]\n",
    "  se <- sqrt(vcovHC(rfit)[2,2])\n",
    "  \n",
    "  cat(sprintf(\"\\nCoef (SE) = %g (%g)\\n\", coef.est, se))\n",
    "  return( list(coef.est=coef.est , se=se, dtil=dtil, ytil=ytil) )\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Parte II: Ejecutar DML (con Cross-Fitting)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- 3.1 Definir los wrappers de los modelos ---\n",
    "\n",
    "# 1. OLS/Logit\n",
    "# glmnet con lambda=0 es OLS\n",
    "yreg_ols <- function(x, y) glmnet(x, y, lambda = 0)\n",
    "# glmnet con family=\"binomial\" es Logit\n",
    "dreg_ols <- function(x, d) glmnet(x, d, family = \"binomial\", lambda = 0)\n",
    "\n",
    "# 2. Lasso\n",
    "yreg_lasso <- function(x, y) rlasso(x, y, post=FALSE) # De hdm\n",
    "# cv.glmnet con alpha=1 es Lasso, CV selecciona lambda.\n",
    "dreg_lasso <- function(x, d) cv.glmnet(x, d, family = \"binomial\", alpha = 1)\n",
    "\n",
    "# 3. Random Forest\n",
    "yreg_rf <- function(x, y) randomForest(x, y) #\n",
    "# d debe ser un factor para que randomForest haga clasificación\n",
    "dreg_rf <- function(x, d) randomForest(x, as.factor(d))\n",
    "\n",
    "# 4. Neural Net (usando nnet)\n",
    "# 'linout=TRUE' es para regresión. 'size' es el nro. de nodos en la capa oculta.\n",
    "# Aumentamos MaxNWts para permitir más pesos (features -> nodos)\n",
    "yreg_nn <- function(x, y) nnet(x, y, size=20, linout=TRUE, trace=FALSE, MaxNWts=2000)\n",
    "dreg_nn <- function(x, d) nnet(x, as.factor(d), size=20, trace=FALSE, MaxNWts=2000)\n",
    "\n",
    "\n",
    "# --- 3.2 Ejecución ---\n",
    "cat(\"\\n--- DML con OLS/Logit ---\\n\")\n",
    "res_ols <- DML.for.PLM(x, d, y, dreg_ols, yreg_ols, nfold=10, \n",
    "                       d_pred_type=\"response\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- DML con Lasso ---\\n\")\n",
    "res_lasso <- DML.for.PLM(x, d, y, dreg_lasso, yreg_lasso, nfold=10, \n",
    "                         d_pred_type=\"response\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- DML con Random Forest ---\\n\")\n",
    "res_rf <- DML.for.PLM(x, d, y, dreg_rf, yreg_rf, nfold=10, \n",
    "                      d_pred_type=\"prob\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- DML con Neural Net ---\\n\")\n",
    "res_nn <- DML.for.PLM(x, d, y, dreg_nn, yreg_nn, nfold=10, \n",
    "                      d_pred_type=\"raw\", y_pred_type=\"raw\")\n",
    "\n",
    "# --- 3.3 Crear Tabla de Resultados (Parte II) ---\n",
    "table_dml <- data.frame(\n",
    "  Estimate = c(res_ols$coef.est, res_lasso$coef.est, res_rf$coef.est, res_nn$coef.est),\n",
    "  `Std. Error` = c(res_ols$se, res_lasso$se, res_rf$se, res_nn$se),\n",
    "  `RMSE Y` = c(sqrt(mean(res_ols$ytil^2)), sqrt(mean(res_lasso$ytil^2)), sqrt(mean(res_rf$ytil^2)), sqrt(mean(res_nn$ytil^2))),\n",
    "  `RMSE D` = c(sqrt(mean(res_ols$dtil^2)), sqrt(mean(res_lasso$dtil^2)), sqrt(mean(res_rf$dtil^2)), sqrt(mean(res_nn$dtil^2))),\n",
    "  row.names = c(\"OLS/Logit\", \"Lasso\", \"Random Forest\", \"NN (nnet)\"),\n",
    "  check.names = FALSE\n",
    ")\n",
    "\n",
    "cat(\"\\n--- Resultados DML (Cross-Fit) ---\\n\")\n",
    "print(table_dml, digits=4)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Parte III: Ejecutar DML Naive (Sin Cross-Fitting)\n",
    "# --------------------------------------------------\n",
    "\n",
    "cat(\"\\n\\n--- Naive DML con OLS/Logit ---\\n\")\n",
    "res_ols_n <- DML.naive(x, d, y, dreg_ols, yreg_ols, \n",
    "                       d_pred_type=\"response\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- Naive DML con Lasso ---\\n\")\n",
    "res_lasso_n <- DML.naive(x, d, y, dreg_lasso, yreg_lasso, \n",
    "                         d_pred_type=\"response\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- Naive DML con Random Forest ---\\n\")\n",
    "res_rf_n <- DML.naive(x, d, y, dreg_rf, yreg_rf, \n",
    "                      d_pred_type=\"prob\", y_pred_type=\"response\")\n",
    "\n",
    "cat(\"\\n--- Naive DML con Neural Net ---\\n\")\n",
    "res_nn_n <- DML.naive(x, d, y, dreg_nn, yreg_nn, \n",
    "                      d_pred_type=\"raw\", y_pred_type=\"raw\")\n",
    "\n",
    "# --- 4.1 Crear Tabla de Resultados (Parte III) ---\n",
    "table_naive <- data.frame(\n",
    "  Estimate = c(res_ols_n$coef.est, res_lasso_n$coef.est, res_rf_n$coef.est, res_nn_n$coef.est),\n",
    "  `Std. Error` = c(res_ols_n$se, res_lasso_n$se, res_rf_n$se, res_nn_n$se),\n",
    "  `RMSE Y` = c(sqrt(mean(res_ols_n$ytil^2)), sqrt(mean(res_lasso_n$ytil^2)), sqrt(mean(res_rf_n$ytil^2)), sqrt(mean(res_nn_n$ytil^2))),\n",
    "  `RMSE D` = c(sqrt(mean(res_ols_n$dtil^2)), sqrt(mean(res_lasso_n$dtil^2)), sqrt(mean(res_rf_n$dtil^2)), sqrt(mean(res_nn_n$dtil^2))),\n",
    "  row.names = c(\"OLS/Logit\", \"Lasso\", \"Random Forest\", \"NN (nnet)\"),\n",
    "  check.names = FALSE\n",
    ")\n",
    "\n",
    "cat(\"\\n--- Resultados Naive DML (Sin Cross-Fit) ---\\n\")\n",
    "print(table_naive, digits=4)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Comparación y Preguntas\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Añadir columna de método\n",
    "table_dml$Method <- \"DML (Cross-Fit)\"\n",
    "table_naive$Method <- \"Naive (No Cross-Fit)\"\n",
    "\n",
    "# Combinar\n",
    "comparison_table <- rbind(table_dml, table_naive)\n",
    "comparison_table <- comparison_table[order(rownames(comparison_table)), ] # Ordenar por nombre de modelo\n",
    "\n",
    "cat(\"\\n\\n--- Comparación Completa: DML vs. Naive ---\\n\")\n",
    "print(comparison_table, digits=4)\n",
    "\n",
    "# --- Respuestas a las Preguntas ---\n",
    "cat(\"\n",
    "### Pregunta 1: What can you say about the RMSE for predicting y and d?\n",
    "\n",
    "Respuesta: Al observar la tabla de comparación, se ve que los RMSE de 'RMSE Y' y 'RMSE D'\n",
    "son consistentemente más bajos para el método 'Naive (No Cross-Fit)' que para 'DML (Cross-Fit)'.\n",
    "Esta diferencia es especialmente grande para los modelos más flexibles como Random Forest y NN.\n",
    "\n",
    "### Pregunta 2: Why is it that estimating with one function yields lower RMSE than another?\n",
    "\n",
    "Respuesta: Esto se debe al **sobreajuste (overfitting)**.\n",
    "- La función **Naive** entrena y evalúa el modelo en los *mismos datos* (error en-muestra o 'in-sample').\n",
    "  Los modelos flexibles (RF, NN) son excelentes para 'memorizar' los datos de entrenamiento,\n",
    "  incluido el ruido, lo que resulta en un RMSE artificialmente bajo.\n",
    "- La función **DML (Cross-Fit)** entrena en un subconjunto y evalúa en un conjunto *no visto*\n",
    "  (error fuera-de-muestra o 'out-of-sample'). Esta es una medida más honesta del rendimiento\n",
    "  real del modelo.\n",
    "\n",
    "### Pregunta 3: What problem would we have if we chose to estimate without cross-fitting?\n",
    "\n",
    "Respuesta: El problema principal es el **sesgo por sobreajuste (overfitting bias)**.\n",
    "\n",
    "La teoría de Double Machine Learning (DML) requiere que los residuos (ytil, dtil) se\n",
    "generen de forma 'ortogonal' al proceso de estimación. El cross-fitting es el\n",
    "mecanismo que lo permite.\n",
    "\n",
    "Si *no* usamos cross-fitting (el método Naive):\n",
    "1. Los modelos se sobreajustan a los datos.\n",
    "2. Los residuos `ytil` y `dtil` están 'contaminados' por este sobreajuste.\n",
    "3. La regresión final de `ytil ~ dtil` estará **sesgada**, y el estimador\n",
    "   del efecto causal no será válido ni confiable.\n",
    "\n",
    "Básicamente, sin cross-fitting, sacrificamos la validez estadística por\n",
    "una falsa sensación de precisión (un RMSE más bajo).\n",
    "\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
