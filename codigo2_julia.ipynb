{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d34005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# 0. Instalación de Paquetes (ejecutar si es necesario)\n",
    "# --------------------------------------------------\n",
    "#=\n",
    "import Pkg\n",
    "\n",
    "# Paquetes estándar de datos y estadísticas\n",
    "Pkg.add([\"CSV\", \"DataFrames\", \"StatsModels\", \"GLM\", \"Random\", \"Downloads\", \"CategoricalArrays\", \"Statistics\", \"PrettyTables\"])\n",
    "\n",
    "# Paquetes de Machine Learning (MLJ)\n",
    "Pkg.add(\"MLJ\")\n",
    "Pkg.add(\"MLJScikitLearnInterface\") # Para OLS, Lasso, RF\n",
    "Pkg.add(\"MLJFlux\") # Para Redes Neuronales\n",
    "Pkg.add(\"Flux\")\n",
    "=#\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Carga de Paquetes\n",
    "# --------------------------------------------------\n",
    "using CSV, DataFrames, StatsModels, GLM, Random, Downloads, CategoricalArrays, Statistics, PrettyTables\n",
    "using MLJ, MLJScikitLearnInterface, MLJFlux, Flux\n",
    "\n",
    "println(\"Paquetes cargados.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Carga y Limpieza de Datos (Parte I)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- 2.1 Cargar Datos ---\n",
    "nombres = [\n",
    "    \"abdt\", \"tg\", \"inuidur1\", \"inuidur2\", \"female\", \"black\", \"hispanic\", \n",
    "    \"othrace\", \"dep\", \"q1\", \"q2\", \"q3\", \"q4\", \"q5\", \"q6\", \"recall\", \n",
    "    \"agelt35\", \"agegt54\", \"durable\", \"nondurable\", \"lusd\", \"husd\", \"muld\"\n",
    "]\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/penn_jae.dat\"\n",
    "file_path = \"penn_jae.dat\"\n",
    "if !isfile(file_path)\n",
    "    Downloads.download(url, file_path)\n",
    "end\n",
    "\n",
    "df = CSV.read(file_path, DataFrame, skipto=2, header=nombres, delim=' ', ignorerepeated=true)\n",
    "println(\"Datos originales: \", size(df))\n",
    "\n",
    "# --- 2.2 Limpieza y Configuración ---\n",
    "\n",
    "# I. Mantener 'tg' == 0 o 4\n",
    "df_cleaned = filter(row -> row.tg == 0 || row.tg == 4, df)\n",
    "println(\"Datos después de filtrar 'tg': \", size(df_cleaned))\n",
    "\n",
    "# II. Definir 'T4' (d)\n",
    "df_cleaned[!, :T4] = (df_cleaned.tg .== 4)\n",
    "\n",
    "# III. Definir 'y' como log('inuidur1')\n",
    "df_cleaned[!, :y] = log.(Float64.(df_cleaned.inuidur1))\n",
    "\n",
    "# Reemplazar -Inf (log(0)) con 'missing'\n",
    "df_cleaned.y = replace(df_cleaned.y, -Inf => missing)\n",
    "\n",
    "# Eliminar filas con 'missing' en 'y'\n",
    "dropmissing!(df_cleaned, [:y])\n",
    "println(\"Datos después de eliminar 'missing' en y: \", size(df_cleaned))\n",
    "\n",
    "# IV. Crear dummies 'dep_1', 'dep_2' (dep_0 es referencia)\n",
    "df_cleaned[!, :dep_1] = (df_cleaned.dep .== 1)\n",
    "df_cleaned[!, :dep_2] = (df_cleaned.dep .== 2)\n",
    "\n",
    "# V. Definir lista de features 'x'\n",
    "feature_list = [\n",
    "    :female, :black, :othrace,\n",
    "    :dep_1, :dep_2,  # :dep_0 es referencia\n",
    "    :q2, :q3, :q4, :q5, :q6, # :q1 es referencia\n",
    "    :recall, :agelt35, :agegt54,\n",
    "    :durable, :nondurable, :lusd, :husd\n",
    "]\n",
    "\n",
    "# Definir x, y, d\n",
    "x = df_cleaned[!, feature_list]\n",
    "y = df_cleaned.y\n",
    "d = Float64.(df_cleaned.T4) # Convertir Boolean a Float64\n",
    "\n",
    "# Coercionar 'x' para MLJ (tratar todo como Continuo)\n",
    "x = coerce(x, Count => Continuous)\n",
    "\n",
    "println(\"Configuración completa.\")\n",
    "println(first(x, 5))\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Definición de Funciones DML\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- 3.1 Cargar todos los modelos de MLJ --- \n",
    "LinearRegressor = @load LinearRegressor pkg=MLJScikitLearnInterface verbosity=0\n",
    "LassoCVRegressor = @load LassoCVRegressor pkg=MLJScikitLearnInterface verbosity=0\n",
    "RandomForestRegressor = @load RandomForestRegressor pkg=MLJScikitLearnInterface verbosity=0\n",
    "NeuralNetworkRegressor = @load NeuralNetworkRegressor pkg=MLJFlux verbosity=0 \n",
    "\n",
    "LogisticClassifier = @load LogisticRegressionCV pkg=MLJScikitLearnInterface verbosity=0\n",
    "# Usamos LogisticRegressionCV para Lasso (L1)\n",
    "LassoClassifier = @load LogisticRegressionCV pkg=MLJScikitLearnInterface verbosity=0 \n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=MLJScikitLearnInterface verbosity=0\n",
    "NeuralNetworkClassifier = @load NeuralNetworkClassifier pkg=MLJFlux verbosity=0\n",
    "\n",
    "# --- 3.2 Función auxiliar del laboratorio para los pliegues ---\n",
    "function training_sample_append(cv_split, test_sample_index)\n",
    "        training_indices = []\n",
    "        for vector in cv_split[Not(test_sample_index)]\n",
    "                training_indices = [training_indices; vector]\n",
    "        end\n",
    "        return training_indices, cv_split[test_sample_index]\n",
    "end\n",
    "\n",
    "# --- 3.3 Función DML (Parte II) ---\n",
    "function dml(x, d, y, modely, modeld, nfold; classifier=true)\n",
    "        n = length(y)\n",
    "        cv = [partition(eachindex(y), fill(1/nfold, nfold-1)..., shuffle = true, rng = 1234)...]\n",
    "        \n",
    "        # Coercionar variables objetivo para MLJ\n",
    "        y_mlj = y\n",
    "        d_mlj = classifier ? categorical(d) : d # d es categórico para clasificadores\n",
    "\n",
    "        machine_y = machine(modely, x, y_mlj, scitype_check_level=0)\n",
    "        machine_d = machine(modeld, x, d_mlj, scitype_check_level=0)\n",
    "        \n",
    "        y_hat = zeros(n)\n",
    "        d_hat = zeros(n)\n",
    "\n",
    "        for fold in 1:nfold\n",
    "                training_fold, test_fold = training_sample_append(cv, fold)\n",
    "                \n",
    "                # Entrenar y predecir Y (Resultado)\n",
    "                MLJ.fit!(machine_y, rows = training_fold)\n",
    "                y_hat[test_fold] = MLJ.predict(machine_y, x[test_fold, :])\n",
    "\n",
    "                # Entrenar y predecir D (Tratamiento)\n",
    "                MLJ.fit!(machine_d, rows = training_fold)\n",
    "                if classifier\n",
    "                    d_hat_probs = MLJ.predict(machine_d, x[test_fold, :])\n",
    "                    # Obtener la probabilidad de la clase '1.0' (equivalente a predict_proba[:, 1])\n",
    "                    d_hat[test_fold] = pdf.(d_hat_probs, 1.0)\n",
    "                else\n",
    "                    d_hat[test_fold] = MLJ.predict(machine_d, x[test_fold, :])\n",
    "                end\n",
    "        end\n",
    "\n",
    "        # Regresión final: resy ~ resd\n",
    "        resy = y .- y_hat\n",
    "        resd = reshape(d .- d_hat, (n, 1))\n",
    "        \n",
    "        ols_data = DataFrame(resy = resy, resd = resd[:, 1])\n",
    "        estimate_model = lm(@formula(resy ~ resd), ols_data)\n",
    "        \n",
    "        # Extraer coeficiente y error estándar para 'resd' (el segundo coeficiente)\n",
    "        coef_est = GLM.coef(estimate_model)[2]\n",
    "        se = GLM.stderror(estimate_model)[2]\n",
    "        \n",
    "        println(\" coef (se) = \", coef_est , \" (\", se, \")\")\n",
    "        return coef_est, se, resy, resd\n",
    "end\n",
    "\n",
    "# --- 3.4 Función DML Naive (Parte III) ---\n",
    "function dml_naive(x, d, y, modely, modeld; classifier=true)\n",
    "        n = length(y)\n",
    "        \n",
    "        y_mlj = y\n",
    "        d_mlj = classifier ? categorical(d) : d\n",
    "\n",
    "        # 1. Entrenar modely en TODOS los datos y predecir EN MUESTRA\n",
    "        machine_y = machine(modely, x, y_mlj, scitype_check_level=0)\n",
    "        MLJ.fit!(machine_y, rows = 1:n)\n",
    "        y_hat = MLJ.predict(machine_y, x)\n",
    "\n",
    "        # 2. Entrenar modeld en TODOS los datos y predecir EN MUESTRA\n",
    "        machine_d = machine(modeld, x, d_mlj, scitype_check_level=0)\n",
    "        MLJ.fit!(machine_d, rows = 1:n)\n",
    "        \n",
    "        d_hat = zeros(n)\n",
    "        if classifier\n",
    "            d_hat_probs = MLJ.predict(machine_d, x)\n",
    "            d_hat = pdf.(d_hat_probs, 1.0)\n",
    "        else\n",
    "            d_hat = MLJ.predict(machine_d, x)\n",
    "        end\n",
    "        \n",
    "        # 3. Calcular residuales (en muestra)\n",
    "        resy = y .- y_hat\n",
    "        resd = reshape(d .- d_hat, (n, 1))\n",
    "\n",
    "        # 4. Regresión final\n",
    "        ols_data = DataFrame(resy = resy, resd = resd[:, 1])\n",
    "        estimate_model = lm(@formula(resy ~ resd), ols_data)\n",
    "        \n",
    "        coef_est = GLM.coef(estimate_model)[2]\n",
    "        se = GLM.stderror(estimate_model)[2]\n",
    "        \n",
    "        println(\" coef (se) = \", coef_est , \" (\", se, \")\")\n",
    "        return coef_est, se, resy, resd\n",
    "end\n",
    "\n",
    "# --- 3.5 Función de Resumen --- \n",
    "function summarize(point, stderr, resy, resd, name)\n",
    "        return DataFrame(\n",
    "                model = [name],\n",
    "                estimate = [point], stderr = [stderr], \n",
    "                rmse_y = [sqrt(mean(resy .^ 2))],\n",
    "                # Asegurarse que resd sea un vector para el cálculo de la media\n",
    "                rmse_d = [sqrt(mean(vec(resd) .^ 2))]\n",
    "        )\n",
    "end\n",
    "\n",
    "println(\"Funciones DML definidas.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Parte II: Ejecutar Debiased ML (con Cross-Fitting)\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- 4.1 Definir Modelos --- \n",
    "\n",
    "# 1. OLS / Logit\n",
    "modely_ols = Standardizer() |> LinearRegressor()\n",
    "modeld_ols = Standardizer() |> LogisticClassifier(cv=5, random_state=123)\n",
    "\n",
    "# 2. Lasso\n",
    "modely_lasso = Standardizer() |> LassoCVRegressor(cv=5, random_state=123)\n",
    "modeld_lasso = Standardizer() |> LassoClassifier(cv=5, penalty=\"l1\", solver=\"liblinear\", random_state=123)\n",
    "\n",
    "# 3. Random Forest\n",
    "# NOTA: RF no necesita estandarización previa\n",
    "modely_rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=123)\n",
    "modeld_rf = RandomForestClassifier(n_estimators=100, min_samples_leaf=5, random_state=123)\n",
    "\n",
    "# 4. Neural Network (NN)\n",
    "# Usamos los hiperparámetros de Python: (50, 20) capas, early_stopping\n",
    "builder_reg = MLJFlux.MLP(hidden=(50, 20), σ=relu)\n",
    "modely_nn = Standardizer() |> NeuralNetworkRegressor(\n",
    "    builder = builder_reg, \n",
    "    epochs = 100, # Python usó max_iter=500, early_stopping. 100-200 epochs es un sustituto razonable.\n",
    "    batch_size = 32,\n",
    "    optimiser = Flux.ADAM(0.001),\n",
    "    rng = 123\n",
    ")\n",
    "\n",
    "builder_clf = MLJFlux.MLP(hidden=(50, 20), σ=relu)\n",
    "modeld_nn = Standardizer() |> NeuralNetworkClassifier(\n",
    "    builder = builder_clf, \n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    optimiser = Flux.ADAM(0.001),\n",
    "    rng = 123\n",
    ")\n",
    "\n",
    "println(\"Modelos definidos. Ejecutando DML (Cross-Fit)...\")\n",
    "\n",
    "# --- 4.2 Ejecutar DML --- \n",
    "println(\"\\n--- OLS/Logit ---\")\n",
    "result_OLS = dml(x, d, y, modely_ols, modeld_ols, 10, classifier=true)\n",
    "table_OLS = summarize(result_OLS..., \"OLS/Logit\")\n",
    "\n",
    "println(\"\\n--- Lasso ---\")\n",
    "result_Lasso = dml(x, d, y, modely_lasso, modeld_lasso, 10, classifier=true)\n",
    "table_Lasso = summarize(result_Lasso..., \"Lasso\")\n",
    "\n",
    "println(\"\\n--- Random Forest ---\")\n",
    "result_RF = dml(x, d, y, modely_rf, modeld_rf, 10, classifier=true)\n",
    "table_RF = summarize(result_RF..., \"Random Forest\")\n",
    "\n",
    "println(\"\\n--- NN (MLP) ---\")\n",
    "result_NN = dml(x, d, y, modely_nn, modeld_nn, 10, classifier=true)\n",
    "table_NN = summarize(result_NN..., \"NN (MLP)\")\n",
    "\n",
    "# --- 4.3 Combinar y Mostrar Tabla --- \n",
    "table_dml = vcat(table_OLS, table_Lasso, table_RF, table_NN)\n",
    "table_dml_sorted = sort(table_dml, [:rmse_y, :rmse_d])\n",
    "\n",
    "println(\"\\n--- Resultados DML (Cross-Fit) --- (RMSE ordenado de menor a mayor)\")\n",
    "pretty_table(table_dml_sorted)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Parte III: Ejecutar DML Naive (Sin Cross-Fitting)\n",
    "# --------------------------------------------------\n",
    "\n",
    "println(\"\\nEjecutando Naive DML (Sin Cross-Fit)...\")\n",
    "\n",
    "println(\"\\n--- OLS/Logit (Naive) ---\")\n",
    "result_OLS_naive = dml_naive(x, d, y, modely_ols, modeld_ols, classifier=true)\n",
    "table_OLS_naive = summarize(result_OLS_naive..., \"OLS/Logit\")\n",
    "\n",
    "println(\"\\n--- Lasso (Naive) ---\")\n",
    "result_Lasso_naive = dml_naive(x, d, y, modely_lasso, modeld_lasso, classifier=true)\n",
    "table_Lasso_naive = summarize(result_Lasso_naive..., \"Lasso\")\n",
    "\n",
    "println(\"\\n--- Random Forest (Naive) ---\")\n",
    "result_RF_naive = dml_naive(x, d, y, modely_rf, modeld_rf, classifier=true)\n",
    "table_RF_naive = summarize(result_RF_naive..., \"Random Forest\")\n",
    "\n",
    "println(\"\\n--- NN (MLP) (Naive) ---\")\n",
    "result_NN_naive = dml_naive(x, d, y, modely_nn, modeld_nn, classifier=true)\n",
    "table_NN_naive = summarize(result_NN_naive..., \"NN (MLP)\")\n",
    "\n",
    "# --- 5.1 Combinar y Mostrar Tabla --- \n",
    "table_naive = vcat(table_OLS_naive, table_Lasso_naive, table_RF_naive, table_NN_naive)\n",
    "table_naive_sorted = sort(table_naive, [:rmse_y, :rmse_d])\n",
    "\n",
    "println(\"\\n--- Resultados Naive DML (Sin Cross-Fit) --- (RMSE ordenado de menor a mayor)\")\n",
    "pretty_table(table_naive_sorted)\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Comparación y Preguntas de Análisis\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --- 6.1 Tabla de Comparación (como se pidió en el chat) ---\n",
    "\n",
    "# Añadir una columna 'Method' a cada tabla\n",
    "table_dml_copy = copy(table_dml)\n",
    "table_naive_copy = copy(table_naive)\n",
    "\n",
    "table_dml_copy[!, :Method] .= \"DML (Cross-Fit)\"\n",
    "table_naive_copy[!, :Method] .= \"Naive (No Cross-Fit)\"\n",
    "\n",
    "# Concatenar\n",
    "comparison = vcat(table_dml_copy, table_naive_copy)\n",
    "\n",
    "# Ordenar por modelo para agruparlos\n",
    "comparison_sorted = sort(comparison, :model)\n",
    "\n",
    "println(\"\\n--- Comparación Completa: DML vs. Naive ---\")\n",
    "pretty_table(comparison_sorted, header_crayon = crayon(bold = true))\n",
    "\n",
    "# --- 6.2 Respuestas a las Preguntas ---\n",
    "\n",
    "println(\"\"\"\n",
    "### Pregunta 1: What can you say about the RMSE for predicting y and d?\n",
    "\n",
    "**Respuesta:** Al observar la tabla de comparación, se ve claramente que los valores de `rmse_y` y `rmse_d` son **consistentemente más bajos** en el método \"Naive (No Cross-Fit)\" que en el método \"DML (Cross-Fit)\". Esta diferencia es mucho más pronunciada en los modelos flexibles como Random Forest y NN.\n",
    "\n",
    "### Pregunta 2: Why is it that estimating with one function yields lower RMSE than another?\n",
    "\n",
    "**Respuesta:** Esto se debe al **sobreajuste (overfitting)**.\n",
    "\n",
    "* La función **Naive (`dml_naive`)** entrena y evalúa el modelo en el *mismo conjunto de datos* (error en-muestra o *in-sample*). Los modelos complejos (especialmente RF y NN) son muy buenos para \"memorizar\" los datos de entrenamiento, incluido el ruido aleatorio. Esto da como resultado un RMSE artificialmente bajo y excesivamente optimista.\n",
    "* La función **DML (`dml`)** usa **cross-fitting (ajuste cruzado)**. Entrena el modelo en una parte de los datos (ej. 9 pliegues) y lo evalúa en una parte *que no ha visto* (el pliegue restante). Este error (fuera-de-muestra o *out-of-sample*) es una medida mucho más honesta y realista del verdadero poder predictivo del modelo en datos nuevos.\n",
    "\n",
    "### Pregunta 3: What problem would we have if we chose to estimate without cross-fitting?\n",
    "\n",
    "**Respuesta:** El problema principal es el **sesgo por sobreajuste (overfitting bias)**.\n",
    "\n",
    "La teoría de DML (Double Machine Learning) requiere que los residuos (`resy` y `resD`) se generen de una manera que sea \"ortogonal\" (estadísticamente independiente) del proceso de estimación. El **cross-fitting** es el mecanismo que nos permite lograr esto.\n",
    "\n",
    "Si *no* usamos cross-fitting (como en `dml_naive`):\n",
    "1.  Los modelos flexibles (RF, NN) se sobreajustarán masivamente a los datos.\n",
    "2.  Los residuos `resy` y `resD` que calculamos estarán \"contaminados\" por este sobreajuste.\n",
    "3.  Cuando ejecutamos la regresión final (`resy ~ resD`), la relación que encontramos estará **sesgada**. El estimador del efecto causal (`estimate`) no será confiable y su inferencia (el `stderr`) será inválida. \n",
    "\n",
    "En resumen, **sin cross-fitting, sacrificamos la validez estadística y la inferencia causal correcta por un falso sentido de precisión (un RMSE más bajo) que proviene del sobreajuste.**\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
