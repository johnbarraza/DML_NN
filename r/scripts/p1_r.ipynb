{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Fitting Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(neuralnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulacion de datos\n",
    "n <- 300\n",
    "x <- seq(0, 2*pi, length.out = n)\n",
    "epsilon <- rnorm(n, 0, 0.1)\n",
    "y <- sin(x) + epsilon\n",
    "\n",
    "datos <- data.frame(x = x, y = y)\n",
    "\n",
    "png(\"simulacion_R.png\", width = 800, height = 400)\n",
    "plot(x, y, pch = 19, col = \"gray\",\n",
    "     main = \"Simulación: y = sin(x) + error\",\n",
    "     xlab = \"x\", ylab = \"y\")\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamiento\n",
    "activaciones <- list(\n",
    "  logistic = \"logistic\",\n",
    "  tanh = \"tanh\"\n",
    ")\n",
    "\n",
    "modelos <- list()\n",
    "predicciones <- list()\n",
    "mses <- c()\n",
    "\n",
    "for (act in names(activaciones)) {\n",
    "  cat(\"\\nEntrenando red con activacion:\", act, \"\\n\")\n",
    "\n",
    "  modelo <- neuralnet(\n",
    "    y ~ x,\n",
    "    data = datos,\n",
    "    hidden = c(20, 20, 20),  # más rápido que 50,50,50\n",
    "    act.fct = activaciones[[act]],\n",
    "    linear.output = TRUE,\n",
    "    stepmax = 1e6\n",
    "  )\n",
    "\n",
    "  modelos[[act]] <- modelo\n",
    "\n",
    "  pred <- compute(modelo, data.frame(x = x))$net.result\n",
    "  predicciones[[act]] <- pred\n",
    "  mses[act] <- mean((pred - y)^2)\n",
    "\n",
    "  png(paste0(\"ajuste_\", act, \"_R.png\"), width = 800, height = 400)\n",
    "  plot(x, y, pch = 19, col = \"gray\",\n",
    "       main = paste(\"Ajuste NN activacion:\", act),\n",
    "       xlab = \"x\", ylab = \"y\")\n",
    "  lines(x, pred, col = \"red\", lwd = 2)\n",
    "  dev.off()\n",
    "}\n",
    "\n",
    "cat(\"\\nMSE obtenidos:\\n\")\n",
    "print(mses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta simulación, la activación **tanh** obtuvo el menor MSE, por lo que corresponde a la red que mejor se ajusta a los datos. Esto refleja que *tanh* modela mejor el patrón suave de la función seno, mientras que *logistic* resulta algo más rígida en los extremos del rango.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este entorno de R no fue posible entrenar redes con activación **ReLU** ni construir una red mixta con funciones distintas por capa. Las librerías disponibles (como `neuralnet`, `nnet` y `RSNNS`) solo aceptan activaciones como *logistic* o *tanh*, y los intentos de usar ReLU generaron errores de convergencia o mensajes del kernel indicando que la función no es válida. Por ello, en R solo se reportan los resultados para *logistic* y *tanh*, mientras que las arquitecturas con ReLU y mezcla de activaciones se implementan en los otros lenguajes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Learning-rate\n",
    "\n",
    "El *learning rate* es un parámetro que controla el tamaño de los pasos con los que una red neuronal ajusta sus pesos durante el entrenamiento. Un valor muy pequeño hace que el aprendizaje sea lento, mientras que uno demasiado grande puede provocar inestabilidad y que la red no converja adecuadamente. Su elección influye directamente en la calidad y la velocidad del ajuste del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulación de datos\n",
    "n <- 300\n",
    "x <- seq(0, 2*pi, length.out = n)\n",
    "epsilon <- rnorm(n, mean = 0, sd = 0.1)\n",
    "y <- sin(x) + epsilon\n",
    "datos <- data.frame(x = x, y = y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(neuralnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs <- c(0.0001, 0.001, 0.01, 0.1)\n",
    "preds <- list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (lr in lrs) {\n",
    "  cat(\"\\nEntrenando NN con learning rate:\", lr, \"\\n\")\n",
    "  modelo <- neuralnet(\n",
    "    y ~ x,\n",
    "    data = datos,\n",
    "    hidden = 50,\n",
    "    act.fct = \"tanh\",\n",
    "    linear.output = TRUE,\n",
    "    learningrate = lr,\n",
    "    stepmax = 1e6\n",
    "  )\n",
    "  pred <- compute(modelo, data.frame(x = x))$net.result\n",
    "  preds[[as.character(lr)]] <- pred\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png(\"learning_rate_R.png\", width = 800, height = 400)\n",
    "plot(x, y, pch = 19, col = \"gray\",\n",
    "     main = \"Comparación de learning rates (1 hidden layer)\",\n",
    "     xlab = \"x\", ylab = \"y\")\n",
    "cols <- c(\"red\", \"blue\", \"green\", \"purple\")\n",
    "i <- 1\n",
    "for (lr in lrs) {\n",
    "  lines(x, preds[[as.character(lr)]], col = cols[i], lwd = 2)\n",
    "  i <- i + 1\n",
    "}\n",
    "legend(\"bottomleft\", legend = paste(\"lr =\", lrs),\n",
    "       col = cols, lwd = 2, bty = \"n\")\n",
    "dev.off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capas <- list(\"2 capas\" = c(50, 50),\n",
    "              \"3 capas\" = c(50, 50, 50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (nombre in names(capas)) {\n",
    "  cat(\"\\n\\n##### Entrenando:\", nombre, \"#####\\n\")\n",
    "  preds_capas <- list()\n",
    "\n",
    "  # Reducir tamaño para evitar que neuralnet colapse\n",
    "  hidden_config <- rep(20, length(capas[[nombre]]))\n",
    "\n",
    "  for (lr in lrs) {\n",
    "    cat(\"Learning rate:\", lr, \"\\n\")\n",
    "\n",
    "    modelo <- neuralnet(\n",
    "      y ~ x,\n",
    "      data = datos,\n",
    "      hidden = hidden_config,\n",
    "      act.fct = \"tanh\",\n",
    "      linear.output = TRUE,\n",
    "      learningrate = lr,\n",
    "      stepmax = 3e5\n",
    "    )\n",
    "\n",
    "    pred <- compute(modelo, data.frame(x = x))$net.result\n",
    "    preds_capas[[as.character(lr)]] <- pred\n",
    "  }\n",
    "\n",
    "  png(paste0(\"learning_rate_\", gsub(\" \", \"_\", nombre), \".png\"),\n",
    "      width = 800, height = 400)\n",
    "\n",
    "  plot(x, y, pch = 19, col = \"gray\",\n",
    "       main = paste(\"Learning rate -\", nombre),\n",
    "       xlab = \"x\", ylab = \"y\")\n",
    "\n",
    "  cols <- c(\"red\", \"blue\", \"green\", \"purple\")\n",
    "  i <- 1\n",
    "  for (lr in lrs) {\n",
    "    lines(x, preds_capas[[as.character(lr)]],\n",
    "          col = cols[i], lwd = 2)\n",
    "    i <- i + 1\n",
    "  }\n",
    "\n",
    "  legend(\"bottomleft\", legend = paste(\"lr =\", lrs),\n",
    "         col = cols, lwd = 2, bty = \"n\")\n",
    "\n",
    "  dev.off()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el desempeño del modelo depende tanto del *learning rate* como de la profundidad de la red. Con un solo nivel oculto, los learning rates intermedios (0.001 y 0.01) producen los ajustes más estables, mientras que tasas muy bajas generan aprendizaje lento y tasas altas provocan oscilaciones. Al aumentar el número de capas a dos y tres, la red se vuelve más sensible: los learning rates altos dejan de funcionar y solo los valores más pequeños (0.0001 o 0.001) logran converger sin inestabilidad. En conjunto, los resultados muestran que redes más profundas requieren learning rates más pequeños para garantizar un entrenamiento estable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
