{
  "Pregunta1_R": {
    "I_FittingData": {
      "Simulacion": {
        "codigo": "set.seed(123)\n\n# simulacion de datos\nn <- 300\nx <- seq(0, 2*pi, length.out = n)\nepsilon <- rnorm(n, 0, 0.1)\ny <- sin(x) + epsilon\n\ndatos <- data.frame(x = x, y = y)\n\npng(\"simulacion_R.png\", width = 800, height = 400)\nplot(x, y, pch = 19, col = \"gray\", main = \"Simulación: y = sin(x) + error\", xlab = \"x\", ylab = \"y\")\ndev.off()"
      },

      "Entrenamiento": {
        "codigo": "# entrenamiento\nactivaciones <- list(\n  logistic = \"logistic\",\n  tanh = \"tanh\"\n)\n\nmodelos <- list()\npredicciones <- list()\nmses <- c()\n\nfor(act in names(activaciones)){\n\n  cat(\"\\nEntrenando red con activacion:\", act, \"\\n\")\n\n  modelo <- neuralnet(\n      y ~ x,\n      data = datos,\n      hidden = c(20,20,20),\n      act.fct = activaciones[[act]],\n      linear.output = TRUE,\n      stepmax = 1e6\n  )\n\n  modelos[[act]] <- modelo\n\n  pred <- compute(modelo, data.frame(x=x))$net.result\n  predicciones[[act]] <- pred\n  mses[act] <- mean((pred - y)^2)\n\n  png(paste0(\"ajuste_\", act, \"_R.png\"), width=800, height=400)\n  plot(x, y, pch=19, col=\"gray\",\n       main=paste(\"Ajuste NN activacion:\", act),\n       xlab=\"x\", ylab=\"y\")\n  lines(x, pred, col=\"red\", lwd=2)\n  dev.off()\n}\n\ncat(\"\\nMSE obtenidos:\\n\")\nprint(mses)"
      },

      "Conclusion": "La activación tanh obtuvo el menor MSE (0.00774), por lo que fue la que mejor se ajustó a los datos. Esto se explica porque tanh puede modelar patrones suaves como la función seno. En este entorno de R no fue posible entrenar redes con activación ReLU ni una red mixta, debido a que las librerías disponibles (neuralnet, nnet, RSNNS) no soportan esa función; los intentos generaron errores del kernel indicando que la activación no es válida."
    },

    "II_LearningRate": {
      "Explicacion": "El learning rate controla el tamaño de los pasos con los que una red neuronal ajusta sus pesos. Un valor muy pequeño produce aprendizaje lento, mientras que uno demasiado grande puede volver inestable la convergencia. Su elección influye directamente en la calidad y velocidad del ajuste.",

      "Simulacion": {
        "codigo": "# simulación de datos\nn <- 300\nx <- seq(0, 2*pi, length.out = n)\nepsilon <- rnorm(n, mean = 0, sd = 0.1)\ny <- sin(x) + epsilon\ndatos <- data.frame(x = x, y = y)"
      },

      "Entrenamiento_1Capa": {
        "codigo": "lrs <- c(0.0001, 0.001, 0.01, 0.1)\npreds <- list()\n\nfor (lr in lrs) {\n  cat(\"\\nEntrenando NN con learning rate:\", lr, \"\\n\")\n  modelo <- neuralnet(\n    y ~ x,\n    data = datos,\n    hidden = 50,\n    act.fct = \"tanh\",\n    linear.output = TRUE,\n    learningrate = lr,\n    stepmax = 1e6\n  )\n  pred <- compute(modelo, data.frame(x=x))$net.result\n  preds[[as.character(lr)]] <- pred\n}\n\npng(\"learning_rate_R.png\", width=800, height=400)\nplot(x, y, pch=19, col=\"gray\",\n     main=\"Comparación de learning rates (1 hidden layer)\",\n     xlab=\"x\", ylab=\"y\")\ncols <- c(\"red\",\"blue\",\"green\",\"purple\")\ni <- 1\nfor (lr in lrs) {\n     lines(x, preds[[as.character(lr)]], col=cols[i], lwd=2)\n     i <- i + 1\n}\nlegend(\"bottomleft\", legend=paste(\"lr =\", lrs), col=cols, lwd=2, bty=\"n\")\ndev.off()"
      },

      "Entrenamiento_MultiplesCapas": {
        "codigo": "capas <- list(\"2 capas\"=c(50,50), \"3 capas\"=c(50,50,50))\n\nfor (nombre in names(capas)) {\n  cat(\"\\n\\n##### Entrenando:\", nombre, \"#####\\n\")\n  preds_capas <- list()\n\n  hidden_config <- rep(20, length(capas[[nombre]]))\n\n  for (lr in lrs) {\n    cat(\"Learning rate:\", lr, \"\\n\")\n\n    modelo <- neuralnet(\n      y ~ x,\n      data = datos,\n      hidden = hidden_config,\n      act.fct = \"tanh\",\n      linear.output = TRUE,\n      learningrate = lr,\n      stepmax = 3e5\n    )\n\n    pred <- compute(modelo, data.frame(x=x))$net.result\n    preds_capas[[as.character(lr)]] <- pred\n  }\n\n  png(paste0(\"learning_rate_\", gsub(\" \", \"_\", nombre), \".png\"), width=800, height=400)\n  plot(x, y, pch=19, col=\"gray\",\n       main=paste(\"Learning rate -\", nombre),\n       xlab=\"x\", ylab=\"y\")\n\n  cols <- c(\"red\",\"blue\",\"green\",\"purple\")\n  i <- 1\n  for (lr in lrs) {\n    lines(x, preds_capas[[as.character(lr)]], col=cols[i], lwd=2)\n    i <- i + 1\n  }\n\n  legend(\"bottomleft\", legend=paste(\"lr =\", lrs), col=cols, lwd=2, bty=\"n\")\n  dev.off()\n}"
      },

      "Conclusion": "El desempeño del modelo depende tanto del learning rate como de la profundidad de la red. Con una sola capa oculta, los learning rates intermedios (0.001 y 0.01) generan los ajustes más estables; valores demasiado bajos aprenden lento y los altos producen oscilaciones. Al aumentar el número de capas, la red se vuelve más sensible: los learning rates altos dejan de funcionar y solo los más pequeños (0.0001 o 0.001) logran converger sin inestabilidad. En conjunto, redes más profundas requieren learning rates más pequeños para garantizar entrenamiento estable."
    }
  }
}
