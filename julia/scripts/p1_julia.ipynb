{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Question 1 – NN Basics\n",
        "\n",
        "## I. Fitting Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "using Random\n",
        "using LinearAlgebra\n",
        "using Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "Random.seed!(123)\n",
        "\n",
        "n = 500\n",
        "x = range(0, 2π, length = n)\n",
        "x_vec = collect(x)\n",
        "\n",
        "ϵ = 0.1 .* randn(n)\n",
        "y = sin.(x_vec) .+ ϵ\n",
        "\n",
        "X = reshape(x_vec, 1, :)\n",
        "Y = reshape(y, 1, :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Funciones de activación\n",
        "function activate(z, act)\n",
        "    if act == :logistic\n",
        "        return 1 ./(1 .+ exp.(-z))\n",
        "    elseif act == :tanh\n",
        "        return tanh.(z)\n",
        "    elseif act == :relu\n",
        "        return max.(0, z)\n",
        "    else\n",
        "        error(\"Activación desconocida\")\n",
        "    end\n",
        "end\n",
        "\n",
        "# Derivadas de las activaciones\n",
        "function activate_prime(z, act)\n",
        "    if act == :logistic\n",
        "        s = 1 ./(1 .+ exp.(-z))\n",
        "        return s .* (1 .- s)\n",
        "    elseif act == :tanh\n",
        "        t = tanh.(z)\n",
        "        return 1 .- t.^2\n",
        "    elseif act == :relu\n",
        "        return (z .> 0) .* 1.0\n",
        "    else\n",
        "        error(\"Activación desconocida\")\n",
        "    end\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Estructura de la red con 3 capas ocultas (50 neuronas cada una)\n",
        "mutable struct SimpleNN\n",
        "    W1::Array{Float64,2}\n",
        "    b1::Array{Float64,2}\n",
        "    W2::Array{Float64,2}\n",
        "    b2::Array{Float64,2}\n",
        "    W3::Array{Float64,2}\n",
        "    b3::Array{Float64,2}\n",
        "    W4::Array{Float64,2}\n",
        "    b4::Array{Float64,2}\n",
        "end\n",
        "\n",
        "# Constructor\n",
        "function SimpleNN(input_dim::Int, hidden_dim::Int, output_dim::Int)\n",
        "    W1 = 0.1 .* randn(hidden_dim, input_dim)\n",
        "    b1 = zeros(hidden_dim, 1)\n",
        "\n",
        "    W2 = 0.1 .* randn(hidden_dim, hidden_dim)\n",
        "    b2 = zeros(hidden_dim, 1)\n",
        "\n",
        "    W3 = 0.1 .* randn(hidden_dim, hidden_dim)\n",
        "    b3 = zeros(hidden_dim, 1)\n",
        "\n",
        "    W4 = 0.1 .* randn(output_dim, hidden_dim)\n",
        "    b4 = zeros(output_dim, 1)\n",
        "\n",
        "    return SimpleNN(W1, b1, W2, b2, W3, b3, W4, b4)\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Propagación hacia adelante\n",
        "function forward(nn::SimpleNN, X, acts::NTuple{3,Symbol})\n",
        "    # Capa 1\n",
        "    z1 = nn.W1 * X .+ nn.b1\n",
        "    a1 = activate(z1, acts[1])\n",
        "\n",
        "    # Capa 2\n",
        "    z2 = nn.W2 * a1 .+ nn.b2\n",
        "    a2 = activate(z2, acts[2])\n",
        "\n",
        "    # Capa 3\n",
        "    z3 = nn.W3 * a2 .+ nn.b3\n",
        "    a3 = activate(z3, acts[3])\n",
        "\n",
        "    # Capa de salida (lineal)\n",
        "    z4 = nn.W4 * a3 .+ nn.b4\n",
        "    ŷ = z4\n",
        "\n",
        "    cache = (z1, a1, z2, a2, z3, a3)\n",
        "    return ŷ, cache\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Entrenamiento por descenso de gradiente\n",
        "using Statistics\n",
        "\n",
        "function train!(nn::SimpleNN, X, Y, acts::NTuple{3,Symbol}; epochs::Int = 500, lr::Float64 = 0.01)\n",
        "    n = size(X, 2)\n",
        "\n",
        "    for epoch in 1:epochs\n",
        "        ŷ, (z1, a1, z2, a2, z3, a3) = forward(nn, X, acts)\n",
        "\n",
        "        # Gradiente capa de salida\n",
        "        δ4 = (2 / n) .* (ŷ .- Y)   # 1 × n\n",
        "        dW4 = δ4 * a3'\n",
        "        db4 = sum(δ4, dims = 2)\n",
        "\n",
        "        # Backprop capas ocultas\n",
        "        δ3 = (nn.W4' * δ4) .* activate_prime(z3, acts[3])\n",
        "        dW3 = δ3 * a2'\n",
        "        db3 = sum(δ3, dims = 2)\n",
        "\n",
        "        δ2 = (nn.W3' * δ3) .* activate_prime(z2, acts[2])\n",
        "        dW2 = δ2 * a1'\n",
        "        db2 = sum(δ2, dims = 2)\n",
        "\n",
        "        δ1 = (nn.W2' * δ2) .* activate_prime(z1, acts[1])\n",
        "        dW1 = δ1 * X'\n",
        "        db1 = sum(δ1, dims = 2)\n",
        "\n",
        "        # Actualización de pesos\n",
        "        nn.W4 .-= lr .* dW4\n",
        "        nn.b4 .-= lr .* db4\n",
        "        nn.W3 .-= lr .* dW3\n",
        "        nn.b3 .-= lr .* db3\n",
        "        nn.W2 .-= lr .* dW2\n",
        "        nn.b2 .-= lr .* db2\n",
        "        nn.W1 .-= lr .* dW1\n",
        "        nn.b1 .-= lr .* db1\n",
        "\n",
        "        if epoch % 100 == 0\n",
        "            loss = mean((ŷ .- Y).^2)\n",
        "            @show epoch loss\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return nn\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Dimensiones de la red\n",
        "input_dim = 1\n",
        "hidden_dim = 50\n",
        "output_dim = 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Activación logística\n",
        "acts_logistic = (:logistic, :logistic, :logistic)\n",
        "nn_logistic = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "train!(nn_logistic, X, Y, acts_logistic; epochs = 600, lr = 0.01)\n",
        "ŷ_logistic, _ = forward(nn_logistic, X, acts_logistic)\n",
        "pred_logistic = vec(ŷ_logistic)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Activación tanh\n",
        "acts_tanh = (:tanh, :tanh, :tanh)\n",
        "nn_tanh = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "train!(nn_tanh, X, Y, acts_tanh; epochs = 600, lr = 0.01)\n",
        "ŷ_tanh, _ = forward(nn_tanh, X, acts_tanh)\n",
        "pred_tanh = vec(ŷ_tanh)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Activación ReLU\n",
        "acts_relu = (:relu, :relu, :relu)\n",
        "nn_relu = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "train!(nn_relu, X, Y, acts_relu; epochs = 600, lr = 0.01)\n",
        "ŷ_relu, _ = forward(nn_relu, X, acts_relu)\n",
        "pred_relu = vec(ŷ_relu)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Activación mixta\n",
        "acts_mixed = (:relu, :tanh, :logistic)\n",
        "nn_mixed = SimpleNN(input_dim, hidden_dim, output_dim)\n",
        "train!(nn_mixed, X, Y, acts_mixed; epochs = 600, lr = 0.01)\n",
        "ŷ_mixed, _ = forward(nn_mixed, X, acts_mixed)\n",
        "pred_mixed = vec(ŷ_mixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Gráfico NN con activación logística\n",
        "p1 = scatter(x_vec, y, label = \"Data\",\n",
        "             title = \"NN con activación logística\",\n",
        "             xlabel = \"x\", ylabel = \"y\")\n",
        "plot!(p1, x_vec, pred_logistic, label = \"Fitted (logistic)\", linewidth = 3)\n",
        "savefig(p1, \"plot_logistic_Julia.png\")\n",
        "display(p1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Gráfico NN con activación tanh\n",
        "p2 = scatter(x_vec, y, label = \"Data\",\n",
        "             title = \"NN con activación tanh\",\n",
        "             xlabel = \"x\", ylabel = \"y\")\n",
        "plot!(p2, x_vec, pred_tanh, label = \"Fitted (tanh)\", linewidth = 3)\n",
        "savefig(p2, \"plot_tanh_Julia.png\")\n",
        "display(p2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Gráfico NN con activación ReLU\n",
        "p3 = scatter(x_vec, y, label = \"Data\",\n",
        "             title = \"NN con activación ReLU\",\n",
        "             xlabel = \"x\", ylabel = \"y\")\n",
        "plot!(p3, x_vec, pred_relu, label = \"Fitted (ReLU)\", linewidth = 3)\n",
        "savefig(p3, \"plot_relu_Julia.png\")\n",
        "display(p3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Gráfico NN con activaciones mixtas\n",
        "p4 = scatter(x_vec, y, label = \"Data\",\n",
        "             title = \"NN con activaciones mixtas (ReLU, tanh, logistic)\",\n",
        "             xlabel = \"x\", ylabel = \"y\")\n",
        "plot!(p4, x_vec, pred_mixed, label = \"Fitted (mixed)\", linewidth = 3)\n",
        "savefig(p4, \"plot_mixta_Julia.png\")\n",
        "display(p4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualmente, la activación tanh es la que produce el ajuste más cercano a la forma real de los datos. Esto se debe a que tanh es suave, no saturante cerca del cero, y permite capturar parte de la curvatura del patrón observado. La activación logística genera un ajuste casi plano, lo cual es esperable porque la sigmoide satura rápidamente y pierde sensibilidad ante variaciones de entrada. La activación ReLU muestra una estructura por tramos lineales, lo que limita su capacidad para representar patrones curvos y continuos. La red con activaciones mixtas mejora ligeramente el desempeño, pero no alcanza la suavidad ni la estabilidad del modelo basado únicamente en tanh.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## II. Learning rate\n",
        "\n",
        "Es un hiperparámetro que controla qué tan grandes son los pasos que da la red neuronal cuando ajusta sus pesos durante el entrenamiento. Un learning rate muy pequeño hace que el aprendizaje avance lentamente y puede que la red no llegue a capturar patrones relevantes. Por su lado, un learning rate muy grande puede provocar inestabilidad, oscilaciones o incluso que la red no aprenda nada. Por eso es importante probar distintos valores y observar cómo cambia el ajuste.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Activaciones (versión compacta)\n",
        "function activate(z, act)\n",
        "    act == :tanh     && return tanh.(z)\n",
        "    act == :logistic && return 1 ./(1 .+ exp.(-z))\n",
        "    act == :relu     && return max.(0, z)\n",
        "    error(\"Activación desconocida\")\n",
        "end\n",
        "\n",
        "function activate_prime(z, act)\n",
        "    act == :tanh && return 1 .- tanh.(z).^2\n",
        "    if act == :logistic\n",
        "        s = 1 ./(1 .+ exp.(-z))\n",
        "        return s .* (1 .- s)\n",
        "    end\n",
        "    act == :relu && return (z .> 0) .* 1.0\n",
        "    error(\"Activación desconocida\")\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Estructura general para k capas ocultas\n",
        "mutable struct NN2\n",
        "    W::Vector{Array{Float64,2}}\n",
        "    b::Vector{Array{Float64,2}}\n",
        "end\n",
        "\n",
        "function build_nn2(input_dim, hidden_dim, output_dim, num_hidden)\n",
        "    W = Vector{Array{Float64,2}}()\n",
        "    b = Vector{Array{Float64,2}}()\n",
        "\n",
        "    # Primera capa\n",
        "    push!(W, 0.1 .* randn(hidden_dim, input_dim))\n",
        "    push!(b, zeros(hidden_dim, 1))\n",
        "\n",
        "    # Capas ocultas adicionales\n",
        "    for _ in 2:num_hidden\n",
        "        push!(W, 0.1 .* randn(hidden_dim, hidden_dim))\n",
        "        push!(b, zeros(hidden_dim, 1))\n",
        "    end\n",
        "\n",
        "    # Capa de salida\n",
        "    push!(W, 0.1 .* randn(output_dim, hidden_dim))\n",
        "    push!(b, zeros(output_dim, 1))\n",
        "\n",
        "    return NN2(W, b)\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Forward para NN2\n",
        "function forward2(nn::NN2, X, act)\n",
        "    Z = []\n",
        "    A = [X]\n",
        "\n",
        "    # Capas ocultas\n",
        "    for ℓ in 1:length(nn.W)-1\n",
        "        z = nn.W[ℓ] * A[end] .+ nn.b[ℓ]\n",
        "        push!(Z, z)\n",
        "        push!(A, activate(z, act))\n",
        "    end\n",
        "\n",
        "    # Capa de salida (lineal)\n",
        "    zL = nn.W[end] * A[end] .+ nn.b[end]\n",
        "    push!(Z, zL)\n",
        "    push!(A, zL)\n",
        "\n",
        "    return A, Z\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Entrenamiento para NN2\n",
        "function train2!(nn::NN2, X, Y, act; epochs=300, lr=0.01)\n",
        "    n = size(X, 2)\n",
        "\n",
        "    for epoch in 1:epochs\n",
        "        A, Z = forward2(nn, X, act)\n",
        "        ŷ = A[end]\n",
        "\n",
        "        δ = (2/n) .* (ŷ .- Y)\n",
        "\n",
        "        dW = Vector{Array{Float64,2}}(undef, length(nn.W))\n",
        "        db = Vector{Array{Float64,2}}(undef, length(nn.W))\n",
        "\n",
        "        for ℓ in reverse(1:length(nn.W))\n",
        "            dW[ℓ] = δ * A[ℓ]'\n",
        "            db[ℓ] = sum(δ, dims = 2)\n",
        "\n",
        "            if ℓ > 1\n",
        "                δ = (nn.W[ℓ]' * δ) .* activate_prime(Z[ℓ-1], act)\n",
        "            end\n",
        "        end\n",
        "\n",
        "        for ℓ in 1:length(nn.W)\n",
        "            nn.W[ℓ] .-= lr .* dW[ℓ]\n",
        "            nn.b[ℓ] .-= lr .* db[ℓ]\n",
        "        end\n",
        "    end\n",
        "\n",
        "    return nn\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Re-simulación de los datos (mismo DGP)\n",
        "Random.seed!(123)\n",
        "n = 500\n",
        "x = range(0, 2π, length=n)\n",
        "x_vec = collect(x)\n",
        "ϵ = 0.1 .* randn(n)\n",
        "y = sin.(x_vec) .+ ϵ\n",
        "X = reshape(x_vec, 1, :)\n",
        "Y = reshape(y, 1, :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 1 hidden layer, distintos learning rates\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "preds_1 = Dict{Float64,Vector{Float64}}()\n",
        "\n",
        "for lr in learning_rates\n",
        "    nn = build_nn2(1, 50, 1, 1)  # 1 hidden layer\n",
        "    train2!(nn, X, Y, :tanh; epochs=500, lr=lr)\n",
        "    A, _ = forward2(nn, X, :tanh)\n",
        "    preds_1[lr] = vec(A[end])\n",
        "end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "p1 = scatter(x_vec, y, label=\"Data\",\n",
        "             title=\"1 Hidden Layer - Diferentes Learning Rates\",\n",
        "             xlabel=\"x\", ylabel=\"y\")\n",
        "\n",
        "for lr in learning_rates\n",
        "    plot!(p1, x_vec, preds_1[lr], label=\"lr=$(lr)\", linewidth=3)\n",
        "end\n",
        "\n",
        "savefig(p1, \"learningrate_1HL_Julia.png\")\n",
        "display(p1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2 hidden layers\n",
        "preds_2 = Dict{Float64,Vector{Float64}}()\n",
        "\n",
        "for lr in learning_rates\n",
        "    nn = build_nn2(1, 50, 1, 2)\n",
        "    train2!(nn, X, Y, :tanh; epochs=500, lr=lr)\n",
        "    A, _ = forward2(nn, X, :tanh)\n",
        "    preds_2[lr] = vec(A[end])\n",
        "end\n",
        "\n",
        "p2 = scatter(x_vec, y, label=\"Data\",\n",
        "             title=\"2 Hidden Layers - Diferentes Learning Rates\",\n",
        "             xlabel=\"x\", ylabel=\"y\")\n",
        "\n",
        "for lr in learning_rates\n",
        "    plot!(p2, x_vec, preds_2[lr], label=\"lr=$(lr)\", linewidth=3)\n",
        "end\n",
        "\n",
        "savefig(p2, \"learningrate_2HL_Julia.png\")\n",
        "display(p2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3 hidden layers\n",
        "preds_3 = Dict{Float64,Vector{Float64}}()\n",
        "\n",
        "for lr in learning_rates\n",
        "    nn = build_nn2(1, 50, 1, 3)\n",
        "    train2!(nn, X, Y, :tanh; epochs=500, lr=lr)\n",
        "    A, _ = forward2(nn, X, :tanh)\n",
        "    preds_3[lr] = vec(A[end])\n",
        "end\n",
        "\n",
        "p3 = scatter(x_vec, y, label=\"Data\",\n",
        "             title=\"3 Hidden Layers - Diferentes Learning Rates\",\n",
        "             xlabel=\"x\", ylabel=\"y\")\n",
        "\n",
        "for lr in learning_rates\n",
        "    plot!(p3, x_vec, preds_3[lr], label=\"lr=$(lr)\", linewidth=3)\n",
        "end\n",
        "\n",
        "savefig(p3, \"learningrate_3HL_Julia.png\")\n",
        "display(p3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados muestran que el learning rate intermedio, especialmente 0.01, ofrece el mejor balance entre velocidad y estabilidad. Los learning rates muy pequeños (0.0001) aprenden demasiado lento y no capturan bien el patrón. Los learning rates grandes (0.1) producen resultados inestables o ajustes incorrectos.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "display_name": "Julia 1.8",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "version": "1.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
